# ECK-Stack-HA

## Objectives:
Deploy a highly available authenticated elasticsearch cluster with budget 300$ for 6 months.

## Challenges Identified
1. **High availablity**
    - Mutiple nodes(at least 3) in multiple zones on region are needed for high availabilty and fault tolerance

2. **Cost**
    - 50$ per month on Google CLoud Platform to run whole stack(VM, storage, data transfer, ip address), with GKE first cluster is an advantage so control plane cost is negated

3. **Performance**
    - Each cluster node need to have good specification to be up and running. Base on initial testing, elasticsearch node need at least 1vCPU to initiate smoothly.

4. **Authentication**
    - Trafic is secured between node and on outside world access to cluster
    
## Solution Overview


To keep the cost of 3 nodes with good spec under $50 per month, **spot VMs** are essential in this case. Kubernetes can automatically reschedule Elasticsearch (ES) nodes if a spot VM is preempted (terminated).

Using Kubernetesâ€™ Elastic Cloud on Kubernetes (ECK) operator simplifies setting up and managing an Elasticsearch cluster. Based on GKE's **regional setup**, 3 nodes is placed in 3 zones, and using **Kubernetes topology spread constraints**, ECK ensures ES nodes are evenly distributed across all zones. This setup improves high availability and fault tolerance, making the cluster resilient to zone failures. Traffic also secure by default when using ECK, data transport between node and node to other service are encrypted with certificate generated by ECK. 

With two node pools config, **spot node pool** and **on-demand backup node pool**, if all spot VMs are preempted at nearly same time, the on-demand backup nodes will automatically scale up to handle the traffic, ensure continuity and minimize downtime. An additional drain job is running as cronjob on k8s cluster, it'll schedule the pods are running on-demand node back to spot node. On-demand nodes's running time are reduced a considerable amount in this case.


- **Detail spec and pricing**:
    - Spot worker nodes(3 t2d-standard-2, true 2 cores CPU and 8GB RAM ) : 27$
    - Boost disk (3 * 30GB) : 9$
    - Persistent disk SSD( 3* 10GB, can be more): 5$
    - Network Load Balancer: 7$
    - Data Egress (10 GB): 1.2$

- **Setup**
    * GKE with terraform

        cd .tf\
        terraform init\
        terraform plan\
        terraform apply
    * Deploy ECK in K8s cluster
        * Install custom resource definition: `kubectl create -f https://download.elastic.co/downloads/eck/2.14.0/crds.yaml`
        * Install operator with its RBAC rules: `kubectl apply -f https://download.elastic.co/downloads/eck/2.14.0/operator.yaml`
        * Verify log: `kubectl -n elastic-system logs -f statefulset.apps/elastic-operator`

    * Deploy Elasticsearch cluster, Kibana, APM server
        * Update each newly created k8s node max heap count using daemon set: `kubectl apply -f ECK/daemon-heapcount.yaml`
        * Install elasticsearch cluster with 3 nodes, each node deploy on different k8s node: `kubectl apply -f ECK/elasticsearch.yaml`
            * Verify working:
                - `PASSWORD=$(kubectl get secret quickstart-es-elastic-user -o go-template='{{.data.elastic | base64decode}}')`
                - `kubectl port-forward svc/quickstart-es-http 9200`
                - `curl -u "elastic:$PASSWORD" -k "https://localhost:9200"`
        * Install kibana: `kubectl apply -f ECK/kibana.yaml`
            * Verify working:
                - `kubectl get service quickstart-kb-http`
                - `kubectl port-forward svc/quickstart-kb-http 5601`
                - Access https://localhost:5601 in browser with self-signed certificate acknowledgement
        * Install APM server: `kubectl apply -f ECK/apm.yaml`

- Deployment notes:
    * K8s cluster network is not in setup scope. It's better to make them private cluster and route all worker egress through NAT gateway.
    * Drain job works with all pods with label `run=elasticsearch` .
    * PodDisruptionBudget config is up by default on elasticsearch ECK deployment, guarantee at least 1 node is working.
    * The situation that all spot nodes are terminated at the same times is quite rare, but to make high availability in highest order, consider running 1 additional backup node.
